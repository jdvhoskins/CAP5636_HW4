{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicit Cart Pole Policy\n",
    "\n",
    "Implement an explicit policy for the cartpole environment without using any learning algorithm. Explain in detail your reasoning behind your policy and run several test episodes to measure its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeff/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Declaration\n",
    "\n",
    "The logic behind this policy is to weight the state variables associated with the pole. Through trial and error, a weight of 0.8 for the angle and 0.2 for the top velocity was found to work well. The input for policy() is the state of the enviroment.\n",
    "\n",
    "state[0] = cart position<br>\n",
    "state[1] = cart velocity<br>\n",
    "state[2] = pole angle<br>\n",
    "state[3] = pole tip velocity<br>\n",
    "\n",
    "action = 0 -> move left<br>\n",
    "action = 1 -> move right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy(state):\n",
    "    weights = np.array([0.8, 0.2])\n",
    "    pole_state = state[2:]\n",
    "    val = np.dot(pole_state, weights)\n",
    "    if val < 0:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = 1\n",
    "    \n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver\n",
    "\n",
    "The run_episode() method takes as inputs the environment and a boolean. The boolean determines if the episode will be rendered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, render):\n",
    "    sleep_timer = 0.01\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    if render:\n",
    "        env.render()\n",
    "        time.sleep(sleep_timer)\n",
    "\n",
    "    for _ in np.arange(200):\n",
    "        action = policy(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(sleep_timer)\n",
    "        if done: break\n",
    "    \n",
    "    if render:\n",
    "        env.close()\n",
    "        print(\"Episode Reward:\", total_reward)\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "The policy is tested by running run_episode() a set number of times and recording the total reward for each episode. The mean reward and its standard deviation are then calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward starts at 0 and is increased by 1 for each action taken to a maximum of 200.\n",
      "\n",
      "Episode Reward: 200.0\n",
      "Episode Reward: 200.0\n",
      "Episode Reward: 200.0\n",
      "Episode Reward: 200.0\n",
      "Episode Reward: 200.0\n",
      "Episode Reward: 200.0\n",
      "Episode Reward: 200.0\n",
      "Episode Reward: 200.0\n",
      "Episode Reward: 200.0\n",
      "Episode Reward: 200.0\n",
      "\n",
      "Average reward      : 200.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "render = True\n",
    "n = 10\n",
    "scores = np.array([])\n",
    "\n",
    "print(\"Reward starts at 0 and is increased by 1 for each action taken to a maximum of 200.\\n\")\n",
    "for _ in np.arange(n):\n",
    "    scores = np.append(scores, [run_episode(env, render)])\n",
    "\n",
    "average_score = np.mean(scores)\n",
    "stdev = np.std(scores)\n",
    "\n",
    "print (\"\\nAverage reward      :\", '{:.2f}'.format(average_score), \"+/-\", '{:.2f}'.format(stdev))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
