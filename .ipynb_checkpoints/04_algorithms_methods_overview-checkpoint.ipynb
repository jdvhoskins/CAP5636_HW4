{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFS\n",
    "\n",
    "Depth first search (DFS) expands the leftmost fringe nodes of a search tree fully before moving on to nodes farther to the right. Assuming there is no cycle in the search tree, DFS is complete. DFS is not optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BFS\n",
    "\n",
    "Breadth first search (BFS) expands the shallowest fringe nodes of a search tree before moving on to deeper nodes. If a solution exists, BFS is complete. BFS is optimal if the cost of transitioning from one state to another is uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCS\n",
    "\n",
    "Uniform cost search (UCS) expands fringe nodes of a search tree based on the backwards cost of the path starting with the least expensive nodes before moving on to costlier nodes. If a solution exists with a finite cost, UCS is complete. UCS is always optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy Search\n",
    "\n",
    "Greedy search looks at the cheapest forward cost estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A\\*\n",
    "\n",
    "A\\* is a combination of UCS and greedy search. heuristic must be less than or equal to actual forward cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimax\n",
    "\n",
    "Minimax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha-Beta Pruning\n",
    "\n",
    "Alpha-beta pruning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectimax\n",
    "\n",
    "Expectimax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "\n",
    "Markov decision processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Value Iteration\n",
    "\n",
    "Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "\n",
    "Policy iteration\n",
    "\n",
    "lecture 9 - slide 39"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online Reinforcement Learning\n",
    "\n",
    "Online reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal Difference Value Learning\n",
    "\n",
    "Temporal difference value learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "Q-learning\n",
    "\n",
    "L 10 - Slide 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Q-Learning\n",
    "\n",
    "Q-learning\n",
    "\n",
    "L 10 - Slide 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Search\n",
    "\n",
    "Q-learning\n",
    "\n",
    "L 10 - Slide 35"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
