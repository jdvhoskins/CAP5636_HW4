{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFS\n",
    "\n",
    "Depth first search (DFS) expands the leftmost fringe nodes of a search tree fully before moving on to nodes farther to the right. Assuming there is no cycle in the search tree, DFS is complete. DFS is not optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BFS\n",
    "\n",
    "Breadth first search (BFS) expands the shallowest fringe nodes of a search tree before moving on to deeper nodes. If a solution exists, BFS is complete. BFS is optimal if the cost of transitioning from one state to another is uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UCS\n",
    "\n",
    "Uniform cost search (UCS) expands fringe nodes of a search tree based on the backwards cost of the path starting with the least expensive nodes before moving on to costlier nodes. If a solution exists with a finite cost, UCS is complete. UCS is always optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy Search\n",
    "\n",
    "Greedy search uses a heuristic to estimate the forward cost of each fringe node. Nodes are expanded in increasing order of the estimated forward cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A\\*\n",
    "\n",
    "A\\* is a combination of UCS and greedy search. To be viable, the heuristic must be less than or equal to actual forward cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minimax\n",
    "\n",
    "Minimax is an adversarial search that seeks to maximize the agent's rewards while minimizing the reward for the adversary. This is done in an alternating fashion as the search tree is traversed. Because the state space is exponential, an exhaustive search is prohibitive. Instead a maximum depth is used to facilitate efficient decision making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alpha-Beta Pruning\n",
    "\n",
    "Alpha-beta pruning is used to further increase the efficiency of minimax searches by removing superfluous computations. If a Max node recieves some value N from one of its children (a Min node), the Min node's siblings can stop evaluating as soon as they find a value lower than N as the Max node will always choose N over those other valuse. The Similar logic, though reversed, applies to Max nodes feeding values to Min Nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectimax\n",
    "\n",
    "While Minimax assumes ideal behavior on the part of the adversary, this is often not accurate. Expectimax addresses this by looking for the expected value based of the adversary's actions. This method requires some knowledge of the probability distribution of the adversaries possible actions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes\n",
    "\n",
    "Markov decision processes are non-deterministic search problems that seek to define a policy based on a set of states, a set of actions, a transition function, and a reward function. The policy yields a decision based only on the current state. The policy is developed based on the sum of discounted rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value Iteration\n",
    "\n",
    "Value iteration optimizes the expected value of each state. These values are used to determine actions by taking the max value over all actions. The policy is not explicitly found, only implicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration\n",
    "\n",
    "Policy iteration assumes a given policy and evaluates one step using one-step look-ahead. The resulting values are used to update the policy an another step is taken. This is repeated untill convergance occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Reinforcement learning uses MDPs but the transition and the reward functions are not known. They must be inferred from trial an error. Offline reinforcement learning yields an explicit policy upon completion, while online reinforcement learning continues to learn as the policy is implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "Q-learning updates the Q-value of each state-action pair by computing a running average of Q-value estimates. Each new sample generates a new estimate to be inculded in the running average. Q-learning is capable of off-policy learning. This occurs when acting suboptimally, wich leads to higher exploration. In order to converge to the optimal policy, the learning rate must be gradually decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approximate Q-Learning\n",
    "\n",
    "Q-learning relies on having an exaustive list of all possible states. As this is often not feasible, learning from a random sampling of states is often done instead. Decisions for unfamiliar states are extrapolated from what is known about similar states."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Search\n",
    "\n",
    "Policy searches often use a linear model to relate various state features. The weights of the linear model effectively make up the policy. These weights can be iteratively changed while checking the performance of the policy for convergance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
