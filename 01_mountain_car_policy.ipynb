{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explicit Mountain Car Policy\n",
    "\n",
    "Implement an explicit policy for the mountain car environment without using any learning algorithm. Explain in detail your reasoning behind your policy and run several test episodes to measure its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeff/.local/lib/python3.6/site-packages/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "env = gym.make(\"MountainCar-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Declaration\n",
    "\n",
    "The logic behind this policy is to always push in the same direction as the velocity, and to work with gravity when the velocity is 0. This poicy efficiently uses the existing momentum of the car to increase the amplitude of the motion over time. The input for policy() is the state of the enviroment.\n",
    "\n",
    "state[0] = car position<br>\n",
    "state[1] = car velocity\n",
    "\n",
    "action = 0 -> push left<br>\n",
    "action = 1 -> no push<br>\n",
    "action = 2 -> push right\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def policy(state):\n",
    "    if state[1] < 0:\n",
    "        action = 0\n",
    "    elif state[1] > 0:\n",
    "            action = 2\n",
    "    elif state[0] > -0.5:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = 2\n",
    "    \n",
    "    return action\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Driver\n",
    "\n",
    "The run_episode() method takes as inputs the environment and a boolean. The boolean determines if the episode will be rendered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, render):\n",
    "    total_reward = 200\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    if render:\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "\n",
    "    for _ in np.arange(200):\n",
    "        action = policy(state)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if render:\n",
    "            env.render()\n",
    "            time.sleep(0.01)\n",
    "        if done: break\n",
    "    \n",
    "    if render:\n",
    "        env.close()\n",
    "        print(\"Episode Reward:\", total_reward)\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n",
    "\n",
    "The policy is tested by running run_episode a set number of times and recording the total reward for each episode. The mean reward and its standard deviation are then calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward has a max of 200 and is decreased by 1 for each step taken.\n",
      "\n",
      "Average reward      : 89.91 +/- 20.10\n",
      "Average steps taken : 110.09\n"
     ]
    }
   ],
   "source": [
    "render = False\n",
    "n = 10000\n",
    "\n",
    "scores = np.array([])\n",
    "\n",
    "print(\"Reward has a max of 200 and is decreased by 1 for each step taken.\")\n",
    "for _ in np.arange(n):\n",
    "    scores = np.append(scores, [run_episode(env, render)])\n",
    "\n",
    "average_score = np.mean(scores)\n",
    "stdev = np.std(scores)\n",
    "\n",
    "print (\"\\nAverage reward      :\", '{:.2f}'.format(average_score), \"+/-\", '{:.2f}'.format(stdev))\n",
    "print (\"Average steps taken :\", '{:.2f}'.format(200 - average_score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
